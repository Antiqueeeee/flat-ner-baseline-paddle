{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cb417e",
   "metadata": {},
   "source": [
    "# 从0.4开始的命名实体识别-序列标注"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb78bc6",
   "metadata": {},
   "source": [
    "## 0 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abd20b",
   "metadata": {},
   "source": [
    "  本篇博文以命名实体识别任务为例，介绍通过神经网络完成任务的整体流程，明确任务中各阶段的目标，为初学者提供实现深度学习算法模型的参考，适合了解过一些NLP相关技术，知道理论知识但苦于无从下手的小伙伴。\n",
    "  区别于如[用深度学习解决nlp中的命名实体识别(ner)问题(深度学习入门项目)](https://cloud.tencent.com/developer/article/1546525)、[命名实体识别NER & 如何使用BERT实现](https://blog.csdn.net/qq_27586341/article/details/103062651)、[huggingface transformers实战系列-04_多语言命名实体识别](https://mp.weixin.qq.com/s/DIV_ACPpjmmr2brVSbqV8w)、[Transformers 库快速入门（六）：序列标注任务](https://xiaosheng.run/2022/03/18/transformers-note-6.html)、[MSRA序列标注实战](https://aistudio.baidu.com/aistudio/projectdetail/3989073)、[基于pytorch的bert_bilstm_crf中文命名实体识别](https://github.com/taishan1994/pytorch_bert_bilstm_crf_ner)等其他命名实体识别相关的优秀文章，本文介绍的重点是构建神经网络解决任务的整体流程，而且不是算法模型各个部分的实现细节，**并未给出解决序列标注问题的常见算法**，希望读者在阅读本文后检索相关资料并自行实现。\n",
    "  为尽量减少读者在数据处理、词向量等环节的工作量，在阅读本篇博文需要读者对BERT预训练模型有一定的认识，并且了解[transformers库的基本用法](https://www.bilibili.com/video/BV1a44y1H7Jc/?spm_id_from=333.999.0.0&vd_source=3269363961c1c9a10f72f01393a219fd)，这也是标题起做“从0.4开始的命名实体识别”而不是“从0开始的命名实体识别”的原因。最后，由于**笔者能力有限**，所提及的内容**仅供思路上的参考**，请读者**切勿照抄照搬**，对于文中、代码中的错误**恳请各位小伙伴以任意形式斧正**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deec83",
   "metadata": {},
   "source": [
    "## 1任务介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96a854",
   "metadata": {},
   "source": [
    "命名实体识别（Named Entities Recognition，NER）是完成NLP下游任务的重要步骤之一，如构建知识图谱（Knowledge Graph，KG）、问答系统（Question and Answering System，QA）等。起初，NER的目标旨在从语句中抽取出三个大类实体、七个小类实体。三大类实体分别为“实体类”、“时间类”、“数字类”；七小类包括：“人名”、“地名”、“组织机构名”、“时间”、“日期”、“货币量”、“百分数”。但随着人工智能（Artificial Intelligence，AI）技术的不断推进与工业界需求的不断升级，NER的抽取目标也不再仅限于通用的“三大类”、“七小类”，开始出现了特定领域内的NER任务，如金融、医疗、司法等领域。针对不同领域、不同类型的语料，不同的NER问题需要采用不同的方法来解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca18cc",
   "metadata": {},
   "source": [
    "在NER任务中，实体大致分为三类：连续实体、嵌套实体、非连续实体。对于连续实体，通常采用的序列标注（Sequence Labeling）的方式解决。序列标注，即为句子中每一个字符都进行标注，语句中的实体字符标注实体标记，非实体部分标为“其他”。数据层面上，对于连续型实体的标注方式通常有“BIO”、“BIOES”两种。“B”来自英文“Begin”，意为实体的开始；“I”取自英文“Inner”，意为实体的内部；“O”为“Other”，表示“其他”，用于标注语料中非实体部分；“E”意为“End”，表示实体结束；“S”取自英文“Single”，意为当前字符可独立作为一个实体。比如对于“Person”类型的实体“李时珍”，“李”为起始字，所以其标记应为“B-Person”；“时”既非实体起始位置，也非实体结束位置，故应将其标注为“I-Person”；汉字“珍”是实体的结束位置，当使用BIO标注框架时，其标注为“I-Person”，但若使用BIOES标注框架时，其标注应为“E-Person”。当实体仅由一个汉字组成时，使用BIO标注的结果为“B-类型”，但使用BIOES进行标注的结果为“S-类型”。此外，还有一种“BMES”标注方式，实际上其与\"BIOES\"标注方式大同小异，其中的“M”与“I”意义相同，对于非实体部分也同样标记为“O”，序列标注的样例如表1所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa250c2",
   "metadata": {},
   "source": [
    "<div align=\"center\">表1 序列标注数据样例</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bcea0",
   "metadata": {},
   "source": [
    "语句|李|时|珍|的|父|亲|李|言|闻\n",
    ":--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "BIO|B-Per|I-Per|I-Per|O|O|O|B-Per|I-Pe|I-Per\n",
    "BIOES|B-Per|I-Per|E-Per|O|O|O|B-Per|I-Per|E-Per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb514cc",
   "metadata": {},
   "source": [
    "## 2任务实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2539c3",
   "metadata": {},
   "source": [
    "神经网络的实现过程大致可以分为数据处理、模型构建、模型训练、模型推理等几个部分。首先需要读取、观察、清洗数据，然后构建算法模型，实现算法的前向传播计算过程并训练模型，最后使用训练好的模型在真实业务场景中的数据进行预测，以完成任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751e5fe",
   "metadata": {},
   "source": [
    "## 2.1数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556fd05",
   "metadata": {},
   "source": [
    "在开始使用神经网络完成目标任务之前，需要将实验用的数据进行预处理，比如去除文本中的特殊符号、空格符等等，避免与任务无关的语料对模型产生过多影响、甚至导致模型出错。数据预处理的质量依赖于研究人员处理数据的相关经验以及对实验数据的了解程度。\n",
    "数据集采用1998年人民日报数据集，数据标注如表12所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b7a81",
   "metadata": {},
   "source": [
    "<div align=\"center\">表2 人民日报数据集标注样例</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9e8ba",
   "metadata": {},
   "source": [
    "文字|标签\n",
    ":--:|:--:|\n",
    "海|O|\n",
    "钓|O|\n",
    "比|O|\n",
    "塞|O|\n",
    "地|O|\n",
    "点|O|\n",
    "在|O|\n",
    "厦|O|\n",
    "门|O|\n",
    "与|O|\n",
    "金|O|\n",
    "门|O|\n",
    "之|O|\n",
    "间|O|\n",
    "的|O|\n",
    "海|O|\n",
    "域|O|\n",
    "。|O|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746e9d7",
   "metadata": {},
   "source": [
    "数据处理阶段需要清洗实验数据并准备后续模型所需要的各种特征。读取实验数据过程中，多使用json库或者python自带的open()方法，这里不对其进行过多赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b18740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer,BertModel,AdamW\n",
    "import pandas as pd\n",
    "import prettytable as pt\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from seqeval.metrics import accuracy_score,f1_score,recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cbb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_path):\n",
    "    origin_text, origin_label = list(), list()\n",
    "    with open(corpus_path, encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "    sent_, tag_ = [], []\n",
    "    for line in lines:\n",
    "        if line != '\\n' and  len(line.strip().split())==2:\n",
    "            [char, label] = line.strip().split()\n",
    "            sent_.append(char)\n",
    "            tag_.append(label)\n",
    "        else:\n",
    "            origin_text.append(sent_)\n",
    "            origin_label.append(tag_)\n",
    "            sent_, tag_ = [], []\n",
    "    return origin_text,origin_label\n",
    "\n",
    "#os.getcwd()获取当前代码所在路径\n",
    "#os.path.join 用来拼接路径，最终得到的data_path为实验数据所在路径\n",
    "#使用os.path.join()可以避免在linux、windows切换时路径报错\n",
    "data_path = os.path.join(os.getcwd(),\"data\",\"参照组\")\n",
    "#读取数据\n",
    "train_text,train_label = read_corpus(os.path.join(data_path,\"example.train\"))\n",
    "dev_text,dev_label = read_corpus(os.path.join(data_path,\"example.dev\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d9c5b",
   "metadata": {},
   "source": [
    "数据清洗的质量依赖于研究人员处理数据的相关经验以及对实验数据的了解程度。通常需要去除语料中与任务不相关的内容，如空格符、emoji等。同时，要注意观察标签是否存在噪音，以序列标注任务为例，可能由于数据标注人员对标注内容理解不统一，使得相同实体对应不同标签，最终导致训练好的模型性能不佳。\n",
    "序列标注任务在清洗数据过程中要尤其注意原始文本与标签的长度保持一致。在原始数据中一个汉字对应一个标签，如果在清洗数据时需要剔除文本中的字符，要注意将其与之对应的标签去除，保证文本长度与标签长度相同。\n",
    "处理过的文本数据和标签以[ [文本1],[文本2],...,[文本n] ]和[ [标注1],[标注2],...,[标注n]]的结构保存，对于普通的序列标注任务来说，后续可以直接以这种形式的数据制作Dataset并封装成DataLoader，但是在进行其他NLP任务时，以这种结构存储数据可能无法满足需求。所以这里我们**绕一圈不必要的弯子**，重新组织一下数据形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cac0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tag2tag(text, label):\n",
    "    # 在序列标注形式的标签中，寻找实体及其类型和索引\n",
    "    # 存放实体相关信息，以字典结构保存，其中包括entity、type以及index\n",
    "    item = dict()\n",
    "    # 保存当前正在读取的实体，实体结束后会存入item[\"entity\"]中\n",
    "    _entity = str()\n",
    "    # ner中存放当前语料包含的所有实体\n",
    "    ner = list()\n",
    "    index = list()\n",
    "    # 遍历序列标注形式的标签，如果当前标签中包含“B-”则表明“上一个实体已经读取完毕，现在开始要开始读取一个新的实体”\n",
    "    # 如果当前标签中包含“I-”，说明正在读取的实体还未结束，将当前标签所对应的字添加进_entity中，继续遍历\n",
    "    # 循环结束后，如果item中不为空，说明存在有未保存的实体，将相关实体信息添加到字典中，最后添加到数据集中。\n",
    "    for i, (t, l) in enumerate(zip(text, label)):\n",
    "        if \"B-\" in l:\n",
    "            if item:\n",
    "                item[\"entity\"] = _entity\n",
    "                item[\"index\"] = index\n",
    "                ner.append(item)\n",
    "                _entity = str()\n",
    "                item = dict()\n",
    "                index = list()\n",
    "            item[\"type\"] = l.split(\"-\")[1]\n",
    "            _entity = t\n",
    "            index.append(i)\n",
    "        if \"I-\" in l and item is not None:\n",
    "            _entity += t\n",
    "            index.append(i)\n",
    "    if item:\n",
    "        item[\"entity\"] = _entity\n",
    "        item[\"index\"] = index\n",
    "        ner.append(item)\n",
    "        _entity = str()\n",
    "        item = dict()\n",
    "        index = list()\n",
    "    return ner\n",
    "\n",
    "def entity_mask_generating(data_item):\n",
    "    _data_item = deepcopy(data_item)\n",
    "    _data_item[\"sentence\"] = list(_data_item[\"sentence\"])\n",
    "    offset = 0\n",
    "    # 为数据中的各个实体分配mask\n",
    "    for ner in _data_item[\"ner\"]:\n",
    "        ner[\"mask\"] = chr(9830 + offset)\n",
    "        for idx in ner[\"index\"]:\n",
    "            _data_item[\"sentence\"][idx] = chr(9830 + offset)\n",
    "        offset += 1\n",
    "    data_item[\"sentence\"] = \"\".join(_data_item[\"sentence\"])\n",
    "    return _data_item\n",
    "\n",
    "def data_reform(data_text,data_label):\n",
    "#     ```\n",
    "#     将数据转换为以下形式  \n",
    "#    [\n",
    "#         {\n",
    "#             \"sentence\": ... ，\n",
    "#             \"ner\" : [\n",
    "#\t\t    \t\t\t{\"entity\": ... , \"type\": ... },\n",
    "#\t\t\t\t\t    {\"entity\": ... , \"type\": ... }  \n",
    "#\t\t\t\t      ]\n",
    "#         },\n",
    "#         ...,\n",
    "#         {...}\n",
    "#     ]\n",
    "#     ```\n",
    "    #存放转换形式后的数据\n",
    "    dataset = list()\n",
    "    #data_text,data_label代表所有数据和所有标签\n",
    "    for text,label in zip(data_text,data_label):\n",
    "        #单条数据\n",
    "        data = dict()\n",
    "        data[\"sentence\"] = \"\".join(text)\n",
    "        data[\"ner\"] = sequence_tag2tag(text,label)\n",
    "        dataset.append(entity_mask_generating(data))\n",
    "    return dataset\n",
    "train_data = data_reform(train_text,train_label)\n",
    "train_data = [data for data in train_data if len(data[\"sentence\"]) < 400 and len(data[\"ner\"])]\n",
    "dev_data = data_reform(dev_text,dev_label)\n",
    "dev_data = [data for data in dev_data if len(data[\"sentence\"]) < 400 and len(data[\"ner\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a65432",
   "metadata": {},
   "source": [
    "此时我们已经将数据处理为：一条数据是一个字典，字典中“sentence”存放文本数据，“ner”中存放文本中出现的实体。在实现其他任务时，还可以在字典里面加入其他特征，比如说词性、相对位置等，也并非一定要拘谨于字典结构，可以根据个人喜好，怎么顺手怎么写。在模型训练过程中，需要将所有任务涉及到的特征都转换成张量形式，所以在数据处理阶段要将所有模型需要的特征准备好。除此之外，在训练过程中还需要计算模型预测与标签之间的“差距”，所以除了特征之外，还需要将标签也存入结构中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064799f",
   "metadata": {},
   "source": [
    "在数据处理阶段不要苦恼将数据处理成什么形式，什么形式都可以。也不用担心是不是多了什么特征、是不是少了什么特征，想起来什么就存什么，后面写着写着发现少了什么，大不了回头补上。就算完全不知道需要用到什么特征，那既然是NLP的相关任务，最起码要有输入的文本对吧？那就先只保存文本，继续完善流程，后面到训练阶段发现缺东西了就回头再补就是，不要担心太多，数据可以变成任何你想要的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43648664",
   "metadata": {},
   "source": [
    "为了保证标签能与输入文本相对齐，我们通过“ner”中包含的实体信息来为文本生成标签。data_reform过程中我们使用entity_mask_generating方法为每个实体都单独生成了特殊符号，我们暂时将其称为“mask”。在后续对sentence进行增加、删减字符的过程中，我们会首先将句子中的**实体替换为同长度的mask**，然后对**sentence**进行**修改**，修改完毕后通过**find实体的mask**来**确定**该**实体**的**索引**。通过这种方式清洗数据，可以避免sentence中同一实体出现多次时，每次find只能找到实体第一次出现的位置，但是这种方法无法在包含嵌套实体的数据集中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635b8ab",
   "metadata": {},
   "source": [
    "下面代码块中entity_mask_replacing方法就是返回某条数据将实体替换成其等长mask的功能；index_correctize则是通过某条数据中的mask位置来重新定位各实体的索引，在数据清洗过程中，流程即为：首先遍历数据集拿到单条数据，使用entity_mask_replacing在sentence中把实体替换为相应mask，然后对sentence进行数据清洗操作，最后通过index_correctize方法从清洗过的sentence中重新定位各实体位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358d82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_mask_replacing(data_item):\n",
    "    #将sentence中的所有实体替换成相应的mask并返回\n",
    "    text = list(deepcopy(data_item[\"sentence\"]))\n",
    "    for ner in data_item[\"ner\"]:\n",
    "        for idx in ner[\"index\"]:\n",
    "            text[idx] = ner[\"mask\"]\n",
    "    text = \"\".join(text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def index_correctize(text,data_item):\n",
    "    #找到data_item中所有实体mask在sentence中的实际位置，并更新ner[\"index\"]\n",
    "    for ner in data_item[\"ner\"]:\n",
    "        _entity = ner[\"entity\"]\n",
    "        _index = text.find(ner[\"mask\"])\n",
    "        if _index != -1:\n",
    "            ner[\"index\"] = list(range(_index,_index + len(_entity)))\n",
    "        else:\n",
    "              raise Exception(f\"{''.join(text)}\\ndidn't found{_entity} in the text.\\norigin:\\n{ner}\")\n",
    "        text = text.replace(ner[\"mask\"]*len(ner[\"entity\"]),ner[\"entity\"])\n",
    "    data_item[\"sentence\"] = text\n",
    "    return data_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2534556",
   "metadata": {},
   "source": [
    "## 2.2数据处理补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08250ca",
   "metadata": {},
   "source": [
    "在对数据进行清洗、整理之前，需要先观察数据，对数据有一定了解。比如统计文本长度、统计实体类型个数、统计每类实体数量分布等等，这些较为容易实现的操作在本文中不会进行过多赘述。\n",
    "1. 查看数据中是否有实体对应多种标签\n",
    "由于数据标注通常都是由多人完成，每个人标注的标准可能存在差异，所以在NER任务中可能存在某个实体对应多种标签，这就属于噪音的一种。这里主要使用Counter库（form collection import Counter），将出现过的实体及其类型以元组的形式保存在集合中，再统计各元组中实体的出现次数，如果出现次数大于1则证明这个实体对应多个实体类型，后续再将标签统一即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a470aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>紫禁城</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美国</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>哥伦比亚</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>中国</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>黄</th>\n",
       "      <td>LOC</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>法国</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>罗马尼亚</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>德</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>英格兰</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>布</th>\n",
       "      <td>LOC</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>日本</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>挪威</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>罗</th>\n",
       "      <td>LOC</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>秘鲁</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>阿根廷</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>奥地利</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>智利</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>保加利亚</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>尼日利亚</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>菲律宾</th>\n",
       "      <td>LOC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>江</th>\n",
       "      <td>PER</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>突尼斯</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>丹麦</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>西班牙</th>\n",
       "      <td>ORG</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>马</th>\n",
       "      <td>LOC</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1\n",
       "紫禁城   ORG  LOC\n",
       "美国    LOC  ORG\n",
       "哥伦比亚  LOC  ORG\n",
       "中国    LOC  ORG\n",
       "黄     LOC  PER\n",
       "法国    LOC  ORG\n",
       "罗马尼亚  LOC  ORG\n",
       "德     LOC  ORG\n",
       "英格兰   LOC  ORG\n",
       "布     LOC  PER\n",
       "日本    ORG  LOC\n",
       "挪威    LOC  ORG\n",
       "美     LOC  ORG\n",
       "罗     LOC  PER\n",
       "秘鲁    LOC  ORG\n",
       "阿根廷   LOC  ORG\n",
       "奥地利   LOC  ORG\n",
       "智利    LOC  ORG\n",
       "保加利亚  ORG  LOC\n",
       "尼日利亚  ORG  LOC\n",
       "菲律宾   LOC  ORG\n",
       "江     PER  LOC\n",
       "突尼斯   ORG  LOC\n",
       "丹麦    ORG  LOC\n",
       "西班牙   ORG  LOC\n",
       "马     LOC  PER"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noise_entity_finding(data_list):\n",
    "    entities = set()\n",
    "    noisy_dict = dict()\n",
    "    for dataset in data_list:\n",
    "        for data_item in dataset:\n",
    "            for ner in data_item[\"ner\"]:\n",
    "                entities.add((ner[\"entity\"],ner[\"type\"]))\n",
    "    entities = list(entities)\n",
    "    entity_appeared = [ent[0] for ent in entities]\n",
    "    result = dict(Counter(entity_appeared))\n",
    "    for key,value in result.items():\n",
    "        if value >= 2 :\n",
    "            _type = [ent[1] for ent in entities if ent[0]== key]\n",
    "            noisy_dict[key] = (_type)\n",
    "    return pd.DataFrame.from_dict(noisy_dict,orient='index')\n",
    "\n",
    "noise_entity_finding([train_data,dev_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193ca13",
   "metadata": {},
   "source": [
    "2. 全角字符转换为半角字符\n",
    "由于transformer的tokenizer在处理文本时，会删除某些全角符号，为保证文本与标签数量一致，所以我们需要将所有文本中的全角符号转换成与之相对应的半角符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f58350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_Q_to_B(data_item):\n",
    "    def Q2B(uchar):\n",
    "        \"\"\"单个字符 全角转半角\"\"\"\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 0x3000:\n",
    "            inside_code = 0x0020\n",
    "        else:\n",
    "            inside_code -= 0xfee0\n",
    "        if inside_code < 0x0020 or inside_code > 0x7e: #转完之后不是半角字符返回原来的字符\n",
    "            return uchar\n",
    "        return chr(inside_code)\n",
    "    converted_text = list()\n",
    "    _text = entity_mask_replacing(data_item)\n",
    "    for txt in _text:\n",
    "        _txt = Q2B(txt)\n",
    "        if len(\"\".join(_txt.split())) >= 1:\n",
    "            converted_text .append(_txt)\n",
    "    converted_text = \"\".join(converted_text) \n",
    "    data_item = index_correctize(converted_text,data_item)\n",
    "    return data_item\n",
    "for data in train_data:\n",
    "    data= conver_Q_to_B(data)\n",
    "for data in dev_data:\n",
    "    data = conver_Q_to_B(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6df1f0",
   "metadata": {},
   "source": [
    "3. 去除文本中不想要的字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9431e8",
   "metadata": {},
   "source": [
    "功能实现的方式十分丑陋，这里主要想展示去除unicode特殊符号的方法。另外，涉及到去除字符的操作要注意校正标签中相应位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ca17fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_noisy_token(dataset):\n",
    "    noisy_token = [\n",
    "    \"【\",\n",
    "    \"】\",\n",
    "    \"&nbsp\",\n",
    "    \"/\",\n",
    "    \"@\",\n",
    "    \"/d\",\n",
    "    ] \n",
    "    for data_item in dataset:\n",
    "        #去除unicode特殊符号\n",
    "        _text = entity_mask_replacing(data_item)\n",
    "        sentence = ''.join(c for c in _text if unicodedata.category(c) != 'Co')\n",
    "        for n in noisy_token:\n",
    "            sentence = sentence.replace(n,\"\")\n",
    "        data_item = index_correctize(sentence,data_item)\n",
    "    return dataset\n",
    "train_data = remove_noisy_token(train_data)\n",
    "dev_data = remove_noisy_token(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc86fed",
   "metadata": {},
   "source": [
    "4.数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71865b",
   "metadata": {},
   "source": [
    "NLP中数据增强的方法有很多种，这里只简单概括一些笔者知道的方法，目的也仅仅是为了让读者知道“有这件事儿”，具体的相关知识还需要读者自行了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c0b38",
   "metadata": {},
   "source": [
    "[**AEDA**](https://arxiv.org/abs/2108.13230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e8be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inert_punctuations(data_item):\n",
    "    PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n",
    "    _sentence = entity_mask_replacing(data_item)\n",
    "    _sentence = \"\".join(_sentence)\n",
    "    #去除句子中后三分之2的索引\n",
    "    insertable_index = list(range(len(_sentence) // 3 ))\n",
    "    #去除句子中实体所在索引\n",
    "    occupied_index = list(set([i for ner in data_item[\"ner\"] for i in ner[\"index\"] ]))\n",
    "    for occ in occupied_index:\n",
    "        try:\n",
    "            insertable_index.remove(occ)\n",
    "        except:\n",
    "            continue\n",
    "    if len(insertable_index) == 0:\n",
    "        selected_loc = 0\n",
    "    else:\n",
    "        selected_loc = random.sample(insertable_index,1)[0]\n",
    "    selected_punctuations = PUNCTUATIONS[random.sample(range(len(PUNCTUATIONS)),1)[0]]\n",
    "    \n",
    "    new_sentence = str()\n",
    "    for i,s in enumerate(_sentence):\n",
    "        if i == selected_loc:\n",
    "            new_sentence += selected_punctuations\n",
    "            new_sentence += s\n",
    "        else:\n",
    "            new_sentence += s\n",
    "    data_item = index_correctize(new_sentence,data_item)\n",
    "    return data_item\n",
    "\n",
    "new_train_data = list()\n",
    "for data in train_data:\n",
    "    new_train_data.append(data)\n",
    "    new_train_data.append(inert_punctuations(data))\n",
    "train_data = new_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50258b79",
   "metadata": {},
   "source": [
    "[**使用相同类型实体替换原实体**](https://zhuanlan.zhihu.com/p/342032812)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee284194",
   "metadata": {},
   "source": [
    "这种数据增强方式的实现方式可以参考[An Analysis of Simple Data Augmentation for Named Entity Recognition](https://github.com/abdulmajee/coling2020-data-augmentation/blob/main/augment.py)。直觉上来说，将句子中出现的实体，替换成与其类型相同的实体后，语义应当仍然通顺，以此来达到数据增强的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6733462",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2ent = dict()\n",
    "for dataset in [train_data,dev_data]:\n",
    "    for data in dataset:\n",
    "        for ner in data[\"ner\"]:\n",
    "            current_type = ner[\"type\"]\n",
    "            if current_type not in type2ent:\n",
    "                type2ent[current_type] = set()\n",
    "            type2ent[current_type].add(ner[\"entity\"])\n",
    "for typ in type2ent:\n",
    "     type2ent[typ] = list(type2ent[typ])\n",
    "        \n",
    "        \n",
    "def entity_augment(data_item):\n",
    "    text = list(deepcopy(data_item[\"sentence\"]))\n",
    "\n",
    "    #将随机替换的实体及相关信息保存到new_ner当中\n",
    "    new_ner = list()\n",
    "    offset = 0\n",
    "    for ner in data_item[\"ner\"]:\n",
    "        #找到与当前实体同类型的实体索引\n",
    "        random_entity_index = random.randint(0, len(type2ent[ner[\"type\"] ]) - 1)\n",
    "        #如果当前存在当前类型实体，且随机得到的实体与原实体不相同，就作为新实体\n",
    "        if type2ent[ner[\"type\"]]:\n",
    "            while type2ent[ner[\"type\"]][random_entity_index] == ner[\"entity\"]:\n",
    "                random_entity_index = random.randint(0, len(type2ent[ner[\"type\"]]))\n",
    "            random_entity = type2ent[ner[\"type\"]][random_entity_index]\n",
    "            \n",
    "            new_ner.append({\n",
    "                \"entity\":random_entity\n",
    "#                 ,\"index\":list(range(ner[\"index\"][0],ner[\"index\"][0] + len(random_entity)))\n",
    "                ,\"type\":ner[\"type\"]\n",
    "                ,\"mask\":chr(8830 + offset)\n",
    "            })\n",
    "            offset += 1\n",
    "    #接下来将文本中的原始实体替换为新生成的实体\n",
    "    #首先把原始实体在原句中替换成相应的mask，\n",
    "    for ner in data_item[\"ner\"]:\n",
    "        for idx in ner[\"index\"]:\n",
    "            text[idx] = ner[\"mask\"]\n",
    "    text = \"\".join(text)\n",
    "    #再把mask换成new_ner中实体的mask，\n",
    "    for i in range(len(data_item[\"ner\"])):\n",
    "        entity_mask = data_item[\"ner\"][i][\"mask\"]*len(data_item[\"ner\"][i][\"entity\"])\n",
    "        new_entity_mask = new_ner[i][\"mask\"]*len(new_ner[i][\"entity\"])\n",
    "        text = text.replace(entity_mask,new_entity_mask)\n",
    "    #重新确定所有mask的索引，把mask替换回实体\n",
    "    for new in new_ner:\n",
    "        _index = text.find(new[\"mask\"] * len(new[\"entity\"]))\n",
    "        if _index != -1:\n",
    "            new[\"index\"] = list(range(_index , _index + len(new[\"entity\"])))\n",
    "            text = text.replace(new[\"mask\"] * len(new[\"entity\"]),new[\"entity\"])\n",
    "        else:\n",
    "            raise Exception(f\"{''.join(text)}\\ndidn't found{_entity} in the text.\\norigin:\\n{ner}\")\n",
    "    return {\"sentence\":text,\"ner\":new_ner}\n",
    "\n",
    "new_train_data = list()\n",
    "for data in train_data:   \n",
    "    new_train_data.append(data)\n",
    "    new_train_data.append(entity_augment(data))\n",
    "train_data = new_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2878df6",
   "metadata": {},
   "source": [
    "经历过数据清洗、数据增强等阶段后，就可以为每条数据生成序列标注需要用到的标签。根据实体的长度决定循环次数，第一次执行循环生成的标签应当以“B-”开头，由于使用的是BIO标注框架，所以剩下的循环中生成的标签均以“I-”开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e738fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generating(data_item):\n",
    "    text = data_item[\"sentence\"]\n",
    "    label = [\"O\"] * len(text)\n",
    "    for ner in data_item[\"ner\"]:\n",
    "        for i in range(len(ner[\"entity\"])):\n",
    "            if i == 0:\n",
    "                label[ner[\"index\"][i]] = \"B-\" + ner[\"type\"]\n",
    "            else:\n",
    "                label[ner[\"index\"][i]] = \"I-\" + ner[\"type\"]\n",
    "    return label\n",
    "for data in train_data:\n",
    "    data[\"label\"] = label_generating(data)\n",
    "for data in dev_data:\n",
    "    data[\"label\"] = label_generating(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088883a",
   "metadata": {},
   "source": [
    "为避免重复运行处理数据的代码，可以将处理好的数据保存到本地，本次案例以json形式保存数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c74c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path,\"train.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data,f,ensure_ascii=False,indent=2)\n",
    "with open(os.path.join(data_path,\"dev.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(dev_data,f,ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46876ad",
   "metadata": {},
   "source": [
    "接下来我们要开始把组织好的数据制作成dataloader，以便后面送入模型，config中保存的是各种设置。读者在执行代码时需要将其中的“bert_name”参数改为自己的预训练模型存储路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0637ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        with open(args.config, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "        self.loss_type = config[\"loss_type\"]\n",
    "        self.dataset = config[\"dataset\"]\n",
    "        self.conv_hid_size = config[\"conv_hid_size\"]\n",
    "        self.bert_hid_size = config[\"bert_hid_size\"]\n",
    "        self.dilation = config[\"dilation\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.bert_learning_rate = config[\"bert_learning_rate\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        \n",
    "        for k, v in args.__dict__.items():\n",
    "            if v is not None:\n",
    "                self.__dict__[k] = v\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}\".format(self.__dict__.items())\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='./config/chinese_news.json')\n",
    "parser.add_argument('--save_path', type=str, default='./outputs')\n",
    "parser.add_argument('--bert_name', type=str, default=r\"bert-base-chinese\")\n",
    "parser.add_argument('--device', type=str, default=\"cuda\")\n",
    "#在notebook中不加这个参数会报错\n",
    "#pycharm中为：args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "config = Config(args)\n",
    "\n",
    "def get_logger(dataset):\n",
    "    pathname = \"./log/{}_{}.txt\".format(dataset, time.strftime(\"%m-%d_%H-%M-%S\"))\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "                                  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    file_handler = logging.FileHandler(pathname)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger(config.dataset)\n",
    "config.logger = logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda268b",
   "metadata": {},
   "source": [
    "模型在预测标签时得到的结果是“数值”，而且送入模型数据的也不能是文本，应该是“数”，所以接下来要做的就是把原始的文本数据，做成“数字”，再封装成DataLoader。不论是语料或者是标签，我们都需要用一个“数字”来代表他们。Transformer中的tokenizer可以帮助我们将语料便捷、高效的转换成“数字”，但是将标签转换为数字这个过程就需要我们自己实现。可以声明一个字典，将每一种标签映射为一个数字，在模型的整个计算中用这个数字来代表这种标签。下面代码块中fill_vocab就是在统计数据中出现了哪些实体类型，将所有出现过的实体类型都映射成一个数字，Vocabulary和fill_vocab的实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3787eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_vocab(vocab, dataset):\n",
    "    entity_num = 0\n",
    "    for data_item in dataset:\n",
    "        for tag in data_item[\"label\"]:\n",
    "            vocab.add_label(tag)\n",
    "            if \"B-\" in tag:\n",
    "                entity_num += 1\n",
    "    return entity_num\n",
    "class Vocabulary(object):\n",
    "    PAD = '<pad>'\n",
    "    UNK = '<unk>'\n",
    "    SUC = '<suc>'\n",
    "    def __init__(self):\n",
    "        self.label2id, self.id2label = {self.PAD : 0},{0 : self.PAD}\n",
    "\n",
    "    def add_label(self, label):\n",
    "        if label not in self.label2id:\n",
    "            self.label2id[label] = len(self.label2id)\n",
    "            self.id2label[self.label2id[label]] = label\n",
    "        assert label == self.id2label[self.label2id[label]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def label_to_id(self, label):\n",
    "        return self.label2id[label]\n",
    "\n",
    "    def id_to_label(self, i):\n",
    "        return self.id2label[i]\n",
    "    def save_Vocabulary(self,save_path):\n",
    "        label2id_path = os.path.join(save_path,\"label2id.json\")\n",
    "        id2label_path = os.path.join(save_path,\"id2label.json\")\n",
    "        with open(label2id_path,\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(self.label2id,f,ensure_ascii=False,indent=2)\n",
    "        with open(id2label_path,\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(self.id2label,f,ensure_ascii=False,indent=2)\n",
    "    def load_Vocabulary(self,save_path):\n",
    "        label2id_path = os.path.join(save_path, \"label2id.json\")\n",
    "        id2label_path = os.path.join(save_path, \"id2label.json\")\n",
    "        with open(label2id_path,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.label2id = json.load(f)\n",
    "        with open(id2label_path,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.id2label = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08991e",
   "metadata": {},
   "source": [
    "接下来读取经过预处理的数据，制作Vocabulary，再经过process_bert方法把文本、标签处理成数值形式，存成Dataset类型，最后把Dataset封装成DataLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c569ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_bert(config):\n",
    "    with open('./data/{}/train.json'.format(config.dataset), 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open('./data/{}/dev.json'.format(config.dataset), 'r', encoding='utf-8') as f:\n",
    "        dev_data = json.load(f)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.bert_name, cache_dir=\"./cache/\")\n",
    "    vocab = Vocabulary()\n",
    "    train_ent_num = fill_vocab(vocab, train_data)\n",
    "    dev_ent_num = fill_vocab(vocab, dev_data)\n",
    "\n",
    "    table = pt.PrettyTable([config.dataset, 'sentences', 'entities'])\n",
    "    table.add_row(['train', len(train_data), train_ent_num])\n",
    "    table.add_row(['dev', len(dev_data), dev_ent_num])\n",
    "    config.logger.info(\"\\n{}\".format(table))\n",
    "    config.label_num = len(vocab.label2id)\n",
    "    vocab.save_Vocabulary(os.path.join(\"./outputs\",config.dataset))\n",
    "    config.vocab = vocab\n",
    "\n",
    "    train_dataset = NERDataset(*process_bert(train_data, tokenizer, vocab))\n",
    "    dev_dataset = NERDataset(*process_bert(dev_data, tokenizer, vocab))\n",
    "\n",
    "    return (train_dataset, dev_dataset), (train_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788c5fa",
   "metadata": {},
   "source": [
    "process_bert的目标是把数据集中的语料和标签都处理成数值形式以供模型进行计算。首先将语料中的每个字符进行tokenize，然后将经过tonkenize的句子convert成ids，并且在语料起始和语料结束两个位置加上CLS和SEP标签，至此就将语料转换成了数值形式。标签序列也以同样的形式处理，使用BERT时要在原始文本加入[CLS]和[SEP]，要注意将label与原始文本对齐，对齐后使用label_to_id将标签转换成对应的数字即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee7fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bert(data, tokenizer, vocab):\n",
    "    bert_inputs = list()\n",
    "    instance_labels = list()\n",
    "    sent_length = list()\n",
    "\n",
    "    for index, instance in enumerate(data):\n",
    "        tokens = [tokenizer.tokenize(word) for word in instance[\"sentence\"]]\n",
    "        pieces = [piece for pieces in tokens for piece in pieces]\n",
    "        _bert_inputs = tokenizer.convert_tokens_to_ids(pieces)\n",
    "        _bert_inputs = np.array([tokenizer.cls_token_id] + _bert_inputs + [tokenizer.sep_token_id])\n",
    "        instance[\"label\"] = [\"O\"] + instance[\"label\"] + [\"O\"]\n",
    "        length = len(instance[\"label\"])\n",
    "        bert_labels = [vocab.label_to_id(tag) for tag in instance[\"label\"]]\n",
    "        bert_inputs.append(_bert_inputs)\n",
    "        instance_labels.append(bert_labels)\n",
    "        sent_length.append(length)\n",
    "    return bert_inputs, instance_labels, sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e569a3",
   "metadata": {},
   "source": [
    "接下来就需要将做好的数值形式的数据封装成张量形式，这里就比较容易理解了。在process_bert中处理好的数据，封装成NERDataset类型，在访问其中元素的时候（调用__getitem__），会将其转换成张量类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6dc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, bert_inputs, bert_labels, sent_length):\n",
    "        self.bert_inputs = bert_inputs\n",
    "        self.bert_labels = bert_labels\n",
    "        self.sent_length = sent_length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.LongTensor(self.bert_inputs[item]), \\\n",
    "               torch.LongTensor(self.bert_labels[item]), \\\n",
    "               self.sent_length[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989416aa",
   "metadata": {},
   "source": [
    "至此处理数据的流程大致就算结束，接下来分别将训练集、验证集、测试集中的数据封装成dataloader。封装时需要设置dataset、batch_size、shuffle等参数，需要特别注意的是需要设置批处理函数collate_fn。在批处理过程中，通常涉及到的操作是padding，即将每个batch中的数据长度统一，任务不同批处理涉及到的操作可能也不同，本次案例的批处理函数实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811a7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    _bert_inputs, _bert_labels, sent_length = map(list, zip(*data))\n",
    "    batch_size = len(_bert_inputs)\n",
    "    bert_inputs = torch.zeros(batch_size,max(sent_length),dtype=torch.long)\n",
    "    bert_labels = torch.zeros(batch_size,max(sent_length),dtype=torch.long)\n",
    "    def fill(data,new_data):\n",
    "        for j, x in enumerate(data):\n",
    "            new_data[j, :x.shape[0]] = x\n",
    "        return new_data\n",
    "\n",
    "    bert_inputs = fill(_bert_inputs, bert_inputs)\n",
    "    bert_labels = fill(_bert_labels, bert_labels)\n",
    "    sent_length = torch.LongTensor(sent_length)\n",
    "    return bert_inputs, bert_labels, sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bb8c0",
   "metadata": {},
   "source": [
    "在NERDataset中保存了每条样本的长度，进行批处理时通过获取当前batch中最大的长度，以决定每条样本padding多个少个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e9e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 15:23:25 - INFO: \n",
      "+--------+-----------+----------+\n",
      "| 参照组 | sentences | entities |\n",
      "+--------+-----------+----------+\n",
      "| train  |   11156   |  30348   |\n",
      "|  dev   |    306    |   854    |\n",
      "+--------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "datasets, ori_data = load_data_bert(config)\n",
    "train_loader, dev_loader  = (\n",
    "        DataLoader(dataset = dataset,\n",
    "                    batch_size = config.batch_size,\n",
    "                    collate_fn = collate_fn,\n",
    "                    shuffle = i == 0,\n",
    "                    #num_workers=4,\n",
    "                    drop_last=i == 0)\n",
    "        for i, dataset in enumerate(datasets)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b33c6d",
   "metadata": {},
   "source": [
    "## 2.3模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5acba0",
   "metadata": {},
   "source": [
    "封装好的DataLoader中我们只保存了文本内容这一项特征作为模型的输入，最终我们期望得到的结果是每个字符对应所有标签的概率分布。我们先以BERT为基础直接加分类层为例，搭建第一个网络模型，模型代码实现如代码块所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbe3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertLinear(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(bertLinear,self).__init__()\n",
    "        self.model_name = \"bertLinear\"\n",
    "        self.config = config\n",
    "        self.bert = BertModel.from_pretrained(config.bert_name,cache_dir=\"./cache/\", output_hidden_states=True)\n",
    "        self.linear = torch.nn.Linear(in_features=self.config.bert_hid_size,out_features=self.config.label_num)\n",
    "    def forward(self,bert_inputs):\n",
    "        try:\n",
    "            outputs = self.bert(bert_inputs,attention_mask=bert_inputs.ne(0).float())\n",
    "            sequence_output , cls_output = outputs[0],outputs[1]\n",
    "            outputs = self.linear(sequence_output)\n",
    "        except:\n",
    "            print(f\"bert_inputs.shape:{bert_inputs.shape}\\nbert_inputs.ne(0).float:{bert_inputs.ne(0).float().shape}\")\n",
    "        \n",
    "        return outputs\n",
    "model = bertLinear(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e5e33",
   "metadata": {},
   "source": [
    "模型初始化就包含三个属性，config、bert以及linear。forward方法中是当前模型前向传播的计算过程，我们通过bert得到文本中每个token的词向量，然后将词向量输入分类层得到该词向量对应所有标签的概率分布。\n",
    "~直接说模型搭建没别的东西了会不会太突兀？~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402406ac",
   "metadata": {},
   "source": [
    "## 2.4Trainer实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057c3c8",
   "metadata": {},
   "source": [
    "模型训练怎么实现？模型验证怎么实现？模型推理怎么实现？在哪里都能听到这三个部分的实现，实际上差不多的言论，但是就是一头雾水。~头上没雾水的请配合下表演，谢谢~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da5870",
   "metadata": {},
   "source": [
    "这里笔者给出一种理解方式以供参考。模型**训练阶段**负责拟合数据，**完成网络模型参数更新**并输出评测指标显示模型训练效果；模型**验证阶段**负责在验证集上**评测模型**训练效果；模型**推理阶段**负责**对未标注数据进行推理预测**，最后将模型预测出的“数值型”的结果转换成真实标签，也可以叫做“解码”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6d785",
   "metadata": {},
   "source": [
    "**模型训练和模型验证的区别**\n",
    "模型训练和模型验证过程中都需要输出评测指标查看模型拟合效果。模型训练和模型验证最主要的一个区别在于验证阶段不需要进行参数更新，所以会经常在别人的代码中看到验证阶段的代码都是在with torch.no_grad()所属的缩进中执行的。另外，在别人代码中你一定见到过model.train()和model.eval()这两句代码，其作用是“是否启用模型中的Batch Normalization 和Dropout”，简单来说就是用来控制模型中部分“零件”是否生效，有的“零件”在training过程中需要用到，但是在验证过程中不能用，这就是模型训练和模型验证的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3bb2e",
   "metadata": {},
   "source": [
    "**模型验证和模型推理的区别**\n",
    "模型验证和模型推理过程中都不要对网络中的参数进行更新，两者的区别在于模型验证过程是有标签的，是可以计算评测指标的，模型推理是在未标注的数据上进行预测的，没有标签不能计算指标，而且还需要将模型预测出的结果转换为真实标签。\n",
    "这时我们可以在脑海中简单脑补一下各个阶段的代码实现，假设我们有一个Trainer类，里面包含模型各个阶段的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db3e76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer(object):\n",
    "#     def __init__(self,model):\n",
    "#         self.model = model\n",
    "#         #巴拉巴拉\n",
    "#     def train(self,data_loader):\n",
    "#         self.model.train()\n",
    "#         for i, data_batch in enumerate(data_loader):\n",
    "#             #巴拉巴拉\n",
    "#         #计算、输出评测指标\n",
    "#     def eval(self,data_loader):\n",
    "#         self.model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for i, data_batch in enumerate(data_loader):\n",
    "#                 #巴拉巴拉\n",
    "#         #计算输出评测指标\n",
    "#     def predict(self,data_loader):\n",
    "#         self.model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for i, data_batch in enumerate(data_loader):\n",
    "#                 #巴拉巴拉\n",
    "#             #模型预测结果解码成真实标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae757d34",
   "metadata": {},
   "source": [
    "为了结构完整，笔者将模型推理也写在了Trainer中，实际上推理的代码应该在另外一个地方实现。因为推理阶段的数据处理等操作应该是与模型训练、模型验证有所不同的。建议读者可以将预测的代码与训练和验证在不同的地方实现。\n",
    "基于上述对模型训练、验证、推理的介绍，接下来开始介绍Trainer的具体实现。Trainer中的任何操作都是以model为基础的，所以在初始化Trainer时最先想到的应该就是model，然后想到更新模型参数需要优化器，模型前向传播完成后还需要损失函数计算loss，那么Trainer的__init__函数应当如代码块所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53684da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, model):\n",
    "#     self.model = model\n",
    "#     criterion = {\n",
    "#         \"ce\": torch.nn.CrossEntropyLoss(),\n",
    "#     }\n",
    "#     self.criterion = criterion[config.loss_type]\n",
    "#     bert_params = set(self.model.bert.parameters())\n",
    "#     other_params = list(set(self.model.parameters()) - bert_params)\n",
    "#     no_decay = ['bias', 'LayerNorm.weight']\n",
    "#     params = [\n",
    "#         {'params': [p for n, p in model.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#          'lr': config.bert_learning_rate,\n",
    "#          'weight_decay': config.weight_decay},\n",
    "#         {'params': [p for n, p in model.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#          'lr': config.bert_learning_rate,\n",
    "#          'weight_decay': 0.0},\n",
    "#         {'params': other_params,\n",
    "#          'lr': config.learning_rate,\n",
    "#          'weight_decay': config.weight_decay},\n",
    "#     ]\n",
    "#     self.optimizer = AdamW(params, lr=config.learning_rate, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ed4f3",
   "metadata": {},
   "source": [
    "接下来开始实现Trainer中的其他功能，首先是模型训练。结合前面的“脑补”，训练部分的逻辑已经很清晰了，访问DataLoader拿到batch数据，输入模型进行计算，然后计算loss，梯度下降方向传播，最后算个评测指标就可以了，实现如代码块所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159030a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(self,epoch,data_loader):\n",
    "#     self.model.train()\n",
    "#     loss_list = list()\n",
    "#     origin_labels = list()\n",
    "#     pred_labels = list()\n",
    "#     #拿batch数据\n",
    "#     for i,data_batch in tqdm(enumerate(data_loader)):\n",
    "#         data_batch = [data.to(config.device) for data in data_batch]\n",
    "#         bert_inputs,bert_labels,sent_length = data_batch\n",
    "#         #输入模型\n",
    "#         outputs = model(bert_inputs, bert_labels)\n",
    "#         #计算损失函数\n",
    "#         loss = self.criterion(\n",
    "#                 outputs.view(-1, config.label_num)[(bert_inputs.ne(0).view(-1)) == 1],\n",
    "#                 bert_labels.view(-1)[(bert_inputs.ne(0).view(-1)) == 1]\n",
    "#             )\n",
    "#         #梯度下降反向传播\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss_list.append(loss.cpu().item())\n",
    "#         #保存输出\n",
    "#         for origin_label,pred_label,bert_input in zip(bert_labels,outputs,bert_inputs):\n",
    "#             origin_label = origin_label[bert_input.ne(0).byte()].cpu().numpy()\n",
    "#             pred_label = torch.argmax(pred_label,-1)[bert_input.ne(0).byte()].cpu().numpy()\n",
    "#             origin_label = [config.vocab.id_to_label(i) for i in origin_label]\n",
    "#             pred_label = [config.vocab.id_to_label(i) for i in pred_label]\n",
    "#             origin_labels.append(origin_label)\n",
    "#             pred_labels.append(pred_label)\n",
    "#     #计算指标\n",
    "#     p = accuracy_score(origin_labels,pred_labels)\n",
    "#     r = recall_score(origin_labels, pred_labels)\n",
    "#     f1 = f1_score(origin_labels, pred_labels)\n",
    "#     table = pt.PrettyTable([\"Train {}\".format(epoch), \"Loss\", \"F1\", \"Precision\", \"Recall\"])\n",
    "#     table.add_row([\"Label\", \"{:.4f}\".format(np.mean(loss_list))] +\n",
    "#                   [\"{:3.4f}\".format(x) for x in [f1, p, r]])\n",
    "#     logger.info(\"\\n{}\".format(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff3b10",
   "metadata": {},
   "source": [
    "相比模型训练，模型验证只要把model.train()换成model.eval()，并且注意模型运算的期间不需要计算梯度，其实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4727f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval(self,epoch,data_loader):\n",
    "#     self.model.eval()\n",
    "#     loss_list = list()\n",
    "#     origin_labels = list()\n",
    "#     pred_labels = list()\n",
    "#     with torch.no_grad():\n",
    "#         #按batch读取数据\n",
    "#         for i, data_batch in tqdm(enumerate(data_loader)):\n",
    "#             data_batch = [data.to(config.device) for data in data_batch]\n",
    "#             bert_inputs, bert_labels, sent_length = data_batch\n",
    "#             #模型计算\n",
    "#             outputs = self.model(bert_inputs)\n",
    "#             #保存原始标签和模型预测出的标签\n",
    "#             for origin_label,pred_label,bert_input in zip(bert_labels,outputs,bert_inputs):\n",
    "#                 origin_label = origin_label[bert_input.ne(0).byte()].cpu().numpy()\n",
    "#                 pred_label = torch.argmax(pred_label,-1)[bert_input.ne(0).byte()].cpu().numpy()\n",
    "#                 origin_label = [config.vocab.id_to_label(i) for i in origin_label]\n",
    "#                 pred_label = [config.vocab.id_to_label(i) for i in pred_label]\n",
    "#                 origin_labels.append(origin_label)\n",
    "#                 pred_labels.append(pred_label)\n",
    "#     #计算指标\n",
    "#     p = accuracy_score(origin_labels,pred_labels)\n",
    "#     r = recall_score(origin_labels, pred_labels)\n",
    "#     f1 = f1_score(origin_labels, pred_labels)\n",
    "#     table = pt.PrettyTable([\"Dev {}\".format(epoch), \"Loss\", \"F1\", \"Precision\", \"Recall\"])\n",
    "#     table.add_row([\"Label\", \"{:.4f}\".format(np.mean(loss_list))] +\n",
    "#                   [\"{:3.4f}\".format(x) for x in [f1, p, r]])\n",
    "#     logger.info(\"\\n{}\".format(table))\n",
    "#     return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1593b4",
   "metadata": {},
   "source": [
    "在序列标注任务中，衡量模型性能主要以F1为主，对于训练过程中性能较好的模型要及时保存，通常在训练开始前设置一个很小的数值作为best_f1，如果当前epoch训练出的模型的F1值大于bert_f1，那么就保存这个模型，在后续模型预测时直接装载已保存的模型进行预测即可，模型的保存与装载如代码块所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ccdea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save(self, path):\n",
    "#     torch.save(self.model.state_dict(), path)\n",
    "# def load(self, path):\n",
    "#     self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73eee4d",
   "metadata": {},
   "source": [
    "至此Trainer的功能基本实现完毕，读者在执行自己任务时可以根据自己的需要对Trainer中的功能进行增减。为保证代码能够顺利运行，我们将上述Trainer的各部分功能组合起来，如代码块所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa94577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        criterion = {\n",
    "            \"ce\": torch.nn.CrossEntropyLoss(),\n",
    "        }\n",
    "        self.criterion = criterion[config.loss_type]\n",
    "        bert_params = set(self.model.bert.parameters())\n",
    "        other_params = list(set(self.model.parameters()) - bert_params)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        params = [\n",
    "            {'params': [p for n, p in model.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': config.bert_learning_rate,\n",
    "             'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in model.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': config.bert_learning_rate,\n",
    "             'weight_decay': 0.0},\n",
    "            {'params': other_params,\n",
    "             'lr': config.learning_rate,\n",
    "             'weight_decay': config.weight_decay},\n",
    "        ]\n",
    "        self.optimizer = AdamW(params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    def train(self, epoch, data_loader):\n",
    "        self.model.train()\n",
    "        loss_list = list()\n",
    "        origin_labels = list()\n",
    "        pred_labels = list()\n",
    "        # 拿batch数据\n",
    "        for i, data_batch in tqdm(enumerate(data_loader)):\n",
    "            data_batch = [data.to(config.device) for data in data_batch]\n",
    "            bert_inputs, bert_labels, sent_length = data_batch\n",
    "            # 输入模型\n",
    "            outputs = self.model(bert_inputs)\n",
    "            # 计算损失函数\n",
    "            loss = self.criterion(\n",
    "                outputs.view(-1, config.label_num)[(bert_inputs.ne(0).view(-1)) == 1],\n",
    "                bert_labels.view(-1)[(bert_inputs.ne(0).view(-1)) == 1]\n",
    "            )\n",
    "            # 梯度下降反向传播\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_list.append(loss.cpu().item())\n",
    "            # 保存输出\n",
    "            for origin_label, pred_label, bert_input in zip(bert_labels, outputs, bert_inputs):\n",
    "                origin_label = origin_label[bert_input.ne(0).byte()].cpu().numpy()\n",
    "                pred_label = torch.argmax(pred_label, -1)[bert_input.ne(0).byte()].cpu().numpy()\n",
    "                origin_label = [config.vocab.id_to_label(i) for i in origin_label]\n",
    "                pred_label = [config.vocab.id_to_label(i) for i in pred_label]\n",
    "                origin_labels.append(origin_label)\n",
    "                pred_labels.append(pred_label)\n",
    "        # 计算指标\n",
    "        p = accuracy_score(origin_labels, pred_labels)\n",
    "        r = recall_score(origin_labels, pred_labels)\n",
    "        f1 = f1_score(origin_labels, pred_labels)\n",
    "        table = pt.PrettyTable([\"Train {}\".format(epoch), \"Loss\", \"F1\", \"Precision\", \"Recall\"])\n",
    "        table.add_row([\"Label\", \"{:.4f}\".format(np.mean(loss_list))] +\n",
    "                      [\"{:3.4f}\".format(x) for x in [f1, p, r]])\n",
    "        logger.info(\"\\n{}\".format(table))\n",
    "    def eval(self, epoch, data_loader):\n",
    "        self.model.eval()\n",
    "#         loss_list = list()\n",
    "        origin_labels = list()\n",
    "        pred_labels = list()\n",
    "        with torch.no_grad():\n",
    "            for i, data_batch in tqdm(enumerate(data_loader)):\n",
    "                data_batch = [data.to(config.device) for data in data_batch]\n",
    "                bert_inputs, bert_labels, sent_length = data_batch\n",
    "                outputs = self.model(bert_inputs)\n",
    "                for origin_label, pred_label, bert_input in zip(bert_labels, outputs, bert_inputs):\n",
    "                    try:\n",
    "                        origin_label = origin_label[bert_input.ne(0).byte()].cpu().numpy()\n",
    "                        pred_label = torch.argmax(pred_label, -1)[bert_input.ne(0).byte()].cpu().numpy()\n",
    "                        origin_label = [config.vocab.id_to_label(i) for i in origin_label]\n",
    "                        pred_label = [config.vocab.id_to_label(i) for i in pred_label]\n",
    "                        origin_labels.append(origin_label)\n",
    "                        pred_labels.append(pred_label)\n",
    "                    except:\n",
    "                        print(f\"bert_input.shape:{bert_input.shape}\\npred_label.shape:{pred_label.shape}\\norigin_label:{origin_label}\")\n",
    "        p = accuracy_score(origin_labels, pred_labels)\n",
    "        r = recall_score(origin_labels, pred_labels)\n",
    "        f1 = f1_score(origin_labels, pred_labels)\n",
    "        table = pt.PrettyTable([\"Dev {}\".format(epoch), \"F1\", \"Precision\", \"Recall\"])\n",
    "        table.add_row([\"Label\"] + [\"{:3.4f}\".format(x) for x in [f1, p, r]])\n",
    "        logger.info(\"\\n{}\".format(table))\n",
    "        return f1\n",
    "    def save(self):\n",
    "        torch.save(\n",
    "            self.model.state_dict(), os.path.join(config.save_path,config.dataset,self.model.model_name + \".pt\")\n",
    "        )\n",
    "\n",
    "    def load(self, path=None):\n",
    "        if path:\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "        else:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(config.save_path, config.dataset, self.model.model_name + \".pt\")\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eed966",
   "metadata": {},
   "source": [
    "## 2.6 模型训练过程实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb32861",
   "metadata": {},
   "source": [
    "各部分功能均已实现完毕，下面开始在调用前面实现好的功能开始训练神经网络。通过load_data_bert读取实验数据并组织成Dataset，然后配置collate_fn和其他参数，将Dataset封装成DataLoader类型，接着实例化神经网络和Trainer工具，最后实现训练过程，每个epoch中先进行一次训练，然后验证模型的训练效果，保存更优的模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "717d4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 20:46:20 - INFO: dict_items([('loss_type', 'ce'), ('dataset', '参照组'), ('conv_hid_size', 96), ('bert_hid_size', 768), ('dilation', [1, 2, 3, 4]), ('epochs', 1), ('batch_size', 2), ('learning_rate', 0.001), ('bert_learning_rate', 5e-05), ('weight_decay', 0), ('config', './config/chinese_news.json'), ('save_path', './outputs'), ('bert_name', 'E:\\\\MyPython\\\\Pre-train-Model\\\\mc-bert-base'), ('device', 'cuda')])\n",
      "2022-11-04 20:46:20 - INFO: dict_items([('loss_type', 'ce'), ('dataset', '参照组'), ('conv_hid_size', 96), ('bert_hid_size', 768), ('dilation', [1, 2, 3, 4]), ('epochs', 1), ('batch_size', 2), ('learning_rate', 0.001), ('bert_learning_rate', 5e-05), ('weight_decay', 0), ('config', './config/chinese_news.json'), ('save_path', './outputs'), ('bert_name', 'E:\\\\MyPython\\\\Pre-train-Model\\\\mc-bert-base'), ('device', 'cuda')])\n",
      "2022-11-04 20:46:20 - INFO: dict_items([('loss_type', 'ce'), ('dataset', '参照组'), ('conv_hid_size', 96), ('bert_hid_size', 768), ('dilation', [1, 2, 3, 4]), ('epochs', 1), ('batch_size', 2), ('learning_rate', 0.001), ('bert_learning_rate', 5e-05), ('weight_decay', 0), ('config', './config/chinese_news.json'), ('save_path', './outputs'), ('bert_name', 'E:\\\\MyPython\\\\Pre-train-Model\\\\mc-bert-base'), ('device', 'cuda')])\n",
      "2022-11-04 20:46:21 - INFO: \n",
      "+--------+-----------+----------+\n",
      "| 参照组 | sentences | entities |\n",
      "+--------+-----------+----------+\n",
      "| train  |   11156   |  30348   |\n",
      "|  dev   |    306    |   854    |\n",
      "+--------+-----------+----------+\n",
      "2022-11-04 20:46:21 - INFO: \n",
      "+--------+-----------+----------+\n",
      "| 参照组 | sentences | entities |\n",
      "+--------+-----------+----------+\n",
      "| train  |   11156   |  30348   |\n",
      "|  dev   |    306    |   854    |\n",
      "+--------+-----------+----------+\n",
      "2022-11-04 20:46:21 - INFO: \n",
      "+--------+-----------+----------+\n",
      "| 参照组 | sentences | entities |\n",
      "+--------+-----------+----------+\n",
      "| train  |   11156   |  30348   |\n",
      "|  dev   |    306    |   854    |\n",
      "+--------+-----------+----------+\n",
      "2022-11-04 20:46:38 - INFO: Building Model\n",
      "2022-11-04 20:46:38 - INFO: Building Model\n",
      "2022-11-04 20:46:38 - INFO: Building Model\n",
      "2022-11-04 20:46:44 - INFO: Epoch: 0\n",
      "2022-11-04 20:46:44 - INFO: Epoch: 0\n",
      "2022-11-04 20:46:44 - INFO: Epoch: 0\n",
      "0it [00:00, ?it/s]D:\\Anaconda3\\envs\\nlife\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "D:\\Anaconda3\\envs\\nlife\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "5578it [06:25, 14.49it/s]\n",
      "D:\\Anaconda3\\envs\\nlife\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "2022-11-04 20:53:11 - INFO: \n",
      "+---------+--------+--------+-----------+--------+\n",
      "| Train 0 |  Loss  |   F1   | Precision | Recall |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "|  Label  | 0.0865 | 0.7789 |   0.9742  | 0.8142 |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "2022-11-04 20:53:11 - INFO: \n",
      "+---------+--------+--------+-----------+--------+\n",
      "| Train 0 |  Loss  |   F1   | Precision | Recall |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "|  Label  | 0.0865 | 0.7789 |   0.9742  | 0.8142 |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "2022-11-04 20:53:11 - INFO: \n",
      "+---------+--------+--------+-----------+--------+\n",
      "| Train 0 |  Loss  |   F1   | Precision | Recall |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "|  Label  | 0.0865 | 0.7789 |   0.9742  | 0.8142 |\n",
      "+---------+--------+--------+-----------+--------+\n",
      "0it [00:00, ?it/s]D:\\Anaconda3\\envs\\nlife\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "D:\\Anaconda3\\envs\\nlife\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "153it [00:02, 71.17it/s]\n",
      "2022-11-04 20:53:14 - INFO: \n",
      "+-------+--------+-----------+--------+\n",
      "| Dev 0 |   F1   | Precision | Recall |\n",
      "+-------+--------+-----------+--------+\n",
      "| Label | 0.8415 |   0.9815  | 0.8548 |\n",
      "+-------+--------+-----------+--------+\n",
      "2022-11-04 20:53:14 - INFO: \n",
      "+-------+--------+-----------+--------+\n",
      "| Dev 0 |   F1   | Precision | Recall |\n",
      "+-------+--------+-----------+--------+\n",
      "| Label | 0.8415 |   0.9815  | 0.8548 |\n",
      "+-------+--------+-----------+--------+\n",
      "2022-11-04 20:53:14 - INFO: \n",
      "+-------+--------+-----------+--------+\n",
      "| Dev 0 |   F1   | Precision | Recall |\n",
      "+-------+--------+-----------+--------+\n",
      "| Label | 0.8415 |   0.9815  | 0.8548 |\n",
      "+-------+--------+-----------+--------+\n",
      "2022-11-04 20:53:15 - INFO: Best DEV F1: 0.8415\n",
      "2022-11-04 20:53:15 - INFO: Best DEV F1: 0.8415\n",
      "2022-11-04 20:53:15 - INFO: Best DEV F1: 0.8415\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='./config/chinese_news.json')\n",
    "parser.add_argument('--save_path', type=str, default='./outputs')\n",
    "parser.add_argument('--bert_name', type=str, default=r\"E:\\MyPython\\Pre-train-Model\\mc-bert-base\")\n",
    "parser.add_argument('--device', type=str, default=\"cuda\")\n",
    "#在notebook中不加这个参数会报错\n",
    "#pycharm中为：args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "config = Config(args)\n",
    "\n",
    "logger = get_logger(config.dataset)\n",
    "logger.info(config)\n",
    "config.logger = logger\n",
    "\n",
    "datasets, ori_data = load_data_bert(config)\n",
    "train_loader, dev_loader = (\n",
    "    DataLoader(dataset = dataset,\n",
    "                batch_size = config.batch_size,\n",
    "                collate_fn = collate_fn,\n",
    "                shuffle = i == 0,\n",
    "                #num_workers=4,\n",
    "                drop_last=i == 0)\n",
    "    for i, dataset in enumerate(datasets)\n",
    ")\n",
    "updates_total = len(datasets[0]) // config.batch_size * config.epochs\n",
    "logger.info(\"Building Model\")\n",
    "model = bertLinear(config).to(config.device)\n",
    "trainer = Trainer(model)\n",
    "\n",
    "best_f1 = 0\n",
    "best_test_f1 = 0\n",
    "#训练config.epochs次\n",
    "for i in range(config.epochs):\n",
    "    logger.info(\"Epoch: {}\".format(i))\n",
    "    #训练模型\n",
    "    trainer.train(i, train_loader)\n",
    "    #训练结束验证训练效果\n",
    "    f1 = trainer.eval(i, dev_loader)\n",
    "    if f1 > best_test_f1:\n",
    "        best_f1 = f1\n",
    "        trainer.save()\n",
    "logger.info(\"Best DEV F1: {:3.4f}\".format(best_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400adf0",
   "metadata": {},
   "source": [
    "## 2.5Predictor实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b6c30",
   "metadata": {},
   "source": [
    "推理阶段中数据处理与训练阶段大致相同，但由于原始测试集中包含了文本及标签，为模拟无标签的场景，故对于测试集的数据仅做了最基本的处理，实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text,test_label = read_corpus(os.path.join(data_path,\"example.test\"))\n",
    "test_data = [{\"sentence\":\"\".join(data)} for data in test_text if len(data) <= 400]\n",
    "with open(os.path.join(data_path,\"test.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data,f,ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0a6f4",
   "metadata": {},
   "source": [
    "笔者认为模型预测部分不应当与训练、验证等方法在同一个地方实现，故在此另起新类“Predictor”，并在其中完成模型预测的相关代码实现。模型预测阶段不需要计算指标，只要注意将模型预测出的结果转换为真实标签即可。由于序列标注形式的标签难以观察抽取效果，所以使用sequence_tag2tag方法将序列标注的结果组织成训练集“ner”，然后再进行保存、展示。后续具体的制作Dataset、封装Dataloader，批处理、及推理过程等步骤请读者自行实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500915de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def sequence_tag2tag(self, text, label):\n",
    "        # 统计序列标注中的实体及其类型\n",
    "        # 存放实体相关信息，以字典结构保存，其中包括entity、type以及index\n",
    "        item = dict()\n",
    "        # 保存当前正在读取的实体，实体结束后会存入item[\"entity\"]中\n",
    "        _entity = str()\n",
    "        # ner中存放当前语料包含的所有实体\n",
    "        ner = list()\n",
    "        index = list()\n",
    "        # 遍历序列标注形式的标签，如果当前标签中包含“B-”则表明“上一个实体已经读取完毕，现在开始要开始读取一个新的实体”\n",
    "        # 如果当前标签中包含“I-”，说明正在读取的实体还未结束，将当前标签所对应的字添加进_entity中，继续遍历\n",
    "        # 循环结束后，如果item中不为空，说明存在有未保存的实体，将相关实体信息添加到字典中，最后添加到数据集中。\n",
    "        for i, (t, l) in enumerate(zip(text, label)):\n",
    "            if \"B-\" in l:\n",
    "                if item:\n",
    "                    item[\"entity\"] = _entity\n",
    "                    item[\"index\"] = index\n",
    "                    ner.append(item)\n",
    "                    _entity = str()\n",
    "                    item = dict()\n",
    "                    index = list()\n",
    "                item[\"type\"] = l.split(\"-\")[1]\n",
    "                _entity = t\n",
    "                index.append(i)\n",
    "            if \"I-\" in l and item is not None:\n",
    "                _entity += t\n",
    "                index.append(i)\n",
    "        if item:\n",
    "            item[\"entity\"] = _entity\n",
    "            item[\"index\"] = index\n",
    "            ner.append(item)\n",
    "            _entity = str()\n",
    "            item = dict()\n",
    "            index = list()\n",
    "        return ner\n",
    "    def predcit(self,data_loader,origin_data):\n",
    "        result = list()\n",
    "        batch = 0\n",
    "        with torch.no_grad():\n",
    "            for data_batch in tqdm(data_loader):\n",
    "                sentence_batch = origin_data[batch : batch + args.batch_size]\n",
    "                data_batch = [data.to(args.device) for data in data_batch]\n",
    "                bert_inputs, sent_length = data_batch\n",
    "                outputs = self.model(bert_inputs)\n",
    "                for sentence, pred_label, bert_input in zip(sentence_batch,outputs,bert_inputs):\n",
    "                    sentence = sentence[\"sentence\"]\n",
    "                    pred_label = torch.argmax(pred_label,-1)[bert_input.ne(0).byte()].cpu().numpy()\n",
    "                    pred_label = [args.vocab.id_to_label(str(i)) for i in pred_label]\n",
    "                    result.append({\"sentence\":sentence,\"label\":self.sequence_tag2tag(sentence,pred_label)})\n",
    "                batch += args.batch_size\n",
    "        with open(os.path.join(args.save_path,args.task,\"model_predicted.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e52d2",
   "metadata": {},
   "source": [
    "# 3.结束语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bea86",
   "metadata": {},
   "source": [
    "希望本文可以帮助读者梳理神经网络的构建流程、消除刚开始接触人工智能的小伙伴对代码实现的“恐惧”。对于其他微调的任务，希望读者可以做到自行修改process_bert方法、自己定义任务所需的Dataset和collate_fn、替换网络模型等，以独立完成任务目标。另外，本文的py形式代码详见[示例代码](https://github.com/Antiqueeeee/Flat-ner-baseline)，其中包含了推理阶段的相关实现，供读者参考。受限于笔者的自身水平，文中、代码中**一定存在疏漏和错误**，请读者切记**不要照抄照搬**，并对文中的错误进行指正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18a498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
